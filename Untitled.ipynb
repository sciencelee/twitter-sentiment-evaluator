{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "widespread-claim",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-ready",
   "metadata": {},
   "source": [
    "#### MY NOTES and research\n",
    "\n",
    "I started this project by doing all of the Deep NLP lessons at Flatiron in the appendix of Module 4\n",
    "This included:\n",
    "- Word embeddings\n",
    "- Word2vec\n",
    "- Sequence Models\n",
    "- Recurrant Neural Networks (RNN)\n",
    "- LSTM (Long Short term Membory) and GRU (Gated Recurrant Unit)\n",
    "\n",
    "I also did some code along and lab with word classifiers.\n",
    "RNN are good for time and sequenced classifications.  They can have vanishing and exploding gradient problems.  The modern equivalent to RNN is LSTM and GRU.  LSTM and GRU 'remember' and 'forget' data by feedback to the model throughout the sequence.  Gates keep only the most important info.  Neither is better, so you should try out both of them for these types of problems.  \n",
    "\n",
    "Other notes:\n",
    "We can train our own model, but there are ones out there that have been trained on all of wikipedia like GloVe which is what we will use here.  You can search for glove.6B.50d.txt to find the file.  It has all the weights and full dictionary of words.\n",
    "\n",
    "It is best to set up pipelines so we can run multiple model types.  We can use traditional tree based, SVC, or logistic regression models.  \n",
    "\n",
    "A Sequential NN with LSTM or GRU will likely work better for more complex tasks but will require more training and tuning time.\n",
    "\n",
    "The input layer has to be a RNN type to deal with sequential data, then you can set up traditional NN (Dense, pooling, dropout etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-reconstruction",
   "metadata": {},
   "source": [
    "### Project guidance\n",
    "The Flatiron curriculum had this as an optional project, and that's why I'm trying it to build some skills.\n",
    "\n",
    "Below is from the instructions:\n",
    "\n",
    "If you choose this option, you'll build an NLP model to analyze Twitter sentiment about Apple and Google products. The dataset comes from CrowdFlower via data.world. Human raters rated the sentiment in over 9,000 Tweets as positive, negative, or neither.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "Build a model that can rate the sentiment of a Tweet based on its content.\n",
    "\n",
    "Aim for a Proof of Concept\n",
    "There are many approaches to NLP problems - start with something simple and iterate from there. For example, you could start by limiting your analysis to positive and negative Tweets only, allowing you to build a binary classifier. Then you could add in the neutral Tweets to build out a multiclass classifier. You may also consider using some of the more advanced NLP methods in the Mod 4 Appendix.\n",
    "\n",
    "\n",
    "#### Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "terminal-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronlee/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "persistent-tobacco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>Hey, Apple fans! Get a peek at the space that'...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5661</th>\n",
       "      <td>RT @mention DELICIOUSLY IRONIC GOOGLE PRIVACY ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>#iPad2 Sold Out, 70% Went to New Buyers - WOW!...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>Google to launch their own social network toda...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030</th>\n",
       "      <td>Apple will be setting up shop @mention #SXSW t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "2647  Hey, Apple fans! Get a peek at the space that'...   \n",
       "5661  RT @mention DELICIOUSLY IRONIC GOOGLE PRIVACY ...   \n",
       "4072  #iPad2 Sold Out, 70% Went to New Buyers - WOW!...   \n",
       "825   Google to launch their own social network toda...   \n",
       "4030  Apple will be setting up shop @mention #SXSW t...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at                             emotion  \n",
       "2647                           Apple                    Positive emotion  \n",
       "5661                          Google                    Negative emotion  \n",
       "4072                            iPad                    Positive emotion  \n",
       "825                              NaN  No emotion toward brand or product  \n",
       "4030                             NaN  No emotion toward brand or product  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv')\n",
    "df = df.sample(frac=0.2)\n",
    "df['emotion'] = df['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "df = df.drop(columns=['is_there_an_emotion_directed_at_a_brand_or_product'])\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-utility",
   "metadata": {},
   "source": [
    "Now, let's transform the dataset\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "Store the column that will be the target, 'category', in the variable target\n",
    "Use the 'tweet_text' column's .map() method to use the word_tokenize function on every piece of text\n",
    "Store the .values attribute from the newly tokenized 'tweet_text' column in the variable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aggressive-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['emotion']\n",
    "data = df['emotion'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-arena",
   "metadata": {},
   "source": [
    "Loading A Pretrained GloVe Model\n",
    "\n",
    "\n",
    "For this project, I will be loading the pretrained weights from GloVe (short for Global Vectors for Word Representation ) from the Stanford NLP Group. \n",
    "\n",
    "These are commonly accepted as some of the best pre-trained word vectors available, and they're open source, so you can get them for free! Even the smallest file is still over 800 MB, so you'll you need to download this file manually.\n",
    "\n",
    "Note that there are several different sizes of pretrained word vectors available for download from the page linked above -- for the purposes of this lesson, you'll only need to use the smallest one, which still contains pretrained word vectors for over 6 billion words and phrases! To download this file, follow the link above and select the file called glove.6b.zip. For simplicity's sake, you can also start the download by clicking this link. You'll be using the GloVe file containing 50-dimensional word vectors for 6 billion words. Once you've downloaded the file, unzip it, and move the file glove.6B.50d.txt into the same directory as this Jupyter notebook.\n",
    "\n",
    "Getting the Total Vocabulary\n",
    "Although the pretrained GloVe data contains vectors for 6 billion words and phrases, you don't need all of them. Instead, you only need the vectors for the words that appear in the dataset. If a word or phrase doesn't appear in the dataset, then there's no reason to waste memory storing the vector for that word or phrase.\n",
    "\n",
    "This means that you need to start by computing the total vocabulary of the dataset. You can do this by adding every word in the dataset into a Python set object. This is easy, since you've already tokenized each comment stored within data.\n",
    "\n",
    "In the cell below, add every token from every comment in data into a set, and store the set in the variable total_vocabulary.\n",
    "\n",
    "HINT: Even though this takes a loop within a loop, you can still do this with a one-line list comprehension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-excellence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
